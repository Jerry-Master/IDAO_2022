#################
### LIBRARIES ###
#################
import pandas as pd
import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings("ignore")


################
###   DATA   ###
################
"""
Description of preprocessed dataset:

Every crystal has at most 3 imperfections. Therefore we simply take the fractional
coordinates of the imperfections. That way the problem is reduced to classifying
triangles. 

Additionally, the variables (d1,d2,d3) represent the length of the sides of the triangle.
"""
filepath = Path('.')
train_coords_dist = pd.read_csv(filepath / 'train_coords_dist.txt', index_col=0)
test_coords_dist = pd.read_csv(filepath / 'test_coords_dist.txt', index_col=0)

public = Path('./data/dichalcogenides_public')
targets = pd.read_csv(public / 'targets.csv', index_col=0)

######################
### LABEL ENCODING ###
######################

def encoder_mapping(row):
    Z1, x1, y1, z1, Z2, x2, y2, z2, Z3, x3, y3, z3, d1, d2, d3 = row 
    Zin = [Z1, Z2, Z3]
    z = [z1, z2, z3]
    Zout = []
    encoder = {0.144826: -1, 0.355174: -1, 0.25: 1, 34.0: -2, 74.0: 2, -1.0: 0}
    for i in range(3):
        Z = Zin[i]
        if Z == 0:
            Zout.append(encoder[z[i]])
        else:
            Zout.append(encoder[Z])
    Z1, Z2, Z3 = Zout
    return pd.DataFrame(data={'Z1': Z1, 'x1': x1, 'y1': y1, 'z1': z1,
                              'Z2': Z2, 'x2': x2, 'y2': y2, 'z2': z2,
                              'Z3': Z3, 'x3': x3, 'y3': y3, 'z3': z3,
                              'd1': d1, 'd2': d2, 'd3': d3}, index=[row.name]).iloc[0]

train_coords_dist2 = train_coords_dist.apply(encoder_mapping, axis=1)
test_coords_dist2 = test_coords_dist.apply(encoder_mapping, axis=1)

#################
### 3-CLASSES ###
#################
"""
Description:

We discovered that the histogram of the target has 3 big values. Which means
that the crystals can be separated into 3 classes. Fortunately, it is possible
to create a perfect classifier for those 3 classes based only on the atomic
numbers of the imperfections.
"""

def classifier_mapping(row):
    Z1, x1, y1, z1, Z2, x2, y2, z2, Z3, x3, y3, z3, d1, d2, d3 = row 
    classifier0 = [set([-2,-2,1]), set([-2,-1,1]), set([-2,1,0]),
               set([-1,-1,1]), set([-1,1,0]), set([1,0,0])]
    classifier1 = [set([-2,-1,0]), set([-1,-1,0]),
                   set([2,-2,-1]), set([2,-1,-1]), set([2,-1,0])]
    classifier2 = [set([-2,-2,0]), set([2,-2,-2]), set([2,-2,0]), set([2,0,0])]
    Z = set([int(Z1), int(Z2), int(Z3)])
    for classifier in classifier0:
        if Z == classifier:
            return 0
    for classifier in classifier1:
        if Z == classifier:
            return 1
    for classifier in classifier2:
        if Z == classifier:
            return 2

target_classes = train_coords_dist2.apply(classifier_mapping, axis=1).sort_index()
test_classes = test_coords_dist2.apply(classifier_mapping, axis=1).sort_index()

#################
### N-CLASSES ###
#################
"""
Description:

Instead of doing regression we converted the problem to one of classification.
To do so, we simply take the bins generated by the histogram and label them.
"""
n, bins, _ = plt.hist(targets, bins=86)
aux = targets.apply(lambda x: np.argwhere(bins <= x[0])[-1][0], axis=1)
keys = set(aux)
values = list(range(len(keys)))

mapping_ = {k:v for k,v in zip(keys,values)}
mapping = lambda x: mapping_[x]

targets_class = aux.apply(mapping)
targets_class = targets_class.reset_index()
targets_class.columns = ['id', 'class']
targets_class = targets_class.set_index('id')

#################
####  MODEL  ####
#################
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier

idx_train0 = target_classes[target_classes==0].index
idx_train1 = target_classes[target_classes==1].index
idx_train2 = target_classes[target_classes==2].index

inv_mapping_ = {v:k for k,v in zip(keys,values)}
inv_mapping = lambda x: inv_mapping_[x]

idx_test0 = test_classes[test_classes==0].index
idx_test1 = test_classes[test_classes==1].index
idx_test2 = test_classes[test_classes==2].index

X_train = train_coords_dist.loc[idx_train0]
y_train = targets_class.loc[idx_train0]

xgb = XGBClassifier(n_estimators=600, learning_rate=0.01, verbosity=0)
lgbm = LGBMClassifier(n_estimators=50, learning_rate=0.1) 
voting = VotingClassifier(estimators=[
     ('xgb', xgb), ('lgbm', lgbm)],
    voting='hard')
voting = voting.fit(X_train, y_train)

predictions0=voting.predict(test_coords_dist.loc[idx_test0])
predictions0=pd.DataFrame(predictions0, columns=['predictions'], index=idx_test0)
predictions0=predictions0['predictions'].apply(inv_mapping)
predictions0=predictions0.apply(lambda x: bins[x])
predictions0=pd.DataFrame(predictions0, columns=['predictions'])

X_train = train_coords_dist.loc[idx_train1]
y_train = targets_class.loc[idx_train1]

xgb = XGBClassifier(n_estimators=600, learning_rate=0.01, verbosity=0)
lgbm = LGBMClassifier(n_estimators=50, learning_rate=0.1) 
voting = VotingClassifier(estimators=[
     ('xgb', xgb), ('lgbm', lgbm)],
    voting='hard')
voting = voting.fit(X_train, y_train)

predictions1=voting.predict(test_coords_dist.loc[idx_test1])
predictions1=pd.DataFrame(predictions1, columns=['predictions'], index=idx_test1)
predictions1=predictions1['predictions'].apply(inv_mapping)
predictions1=predictions1.apply(lambda x: bins[x])
predictions1=pd.DataFrame(predictions1, columns=['predictions'])

X_train = train_coords_dist.loc[idx_train2]
y_train = targets_class.loc[idx_train2]

xgb = XGBClassifier(n_estimators=6, learning_rate=0.1, verbosity=0)
xgb = xgb.fit(X_train, y_train)

predictions2=xgb.predict(test_coords_dist.loc[idx_test2])
predictions2=pd.DataFrame(predictions2, columns=['predictions'], index=idx_test2)
predictions2=predictions2['predictions'].apply(inv_mapping)
predictions2=predictions2.apply(lambda x: bins[x])
predictions2=pd.DataFrame(predictions2, columns=['predictions'])
predictions2[:] = 1.808325 # This class has only one value, the model is useless.

predictions = pd.concat([predictions0,predictions1,predictions2])
predictions = predictions.reset_index()
predictions.columns = ['id', 'predictions']
predictions.to_csv('submission.csv', index=False)